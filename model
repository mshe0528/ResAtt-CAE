# import torch
# import torch.nn as nn
# import torch.optim as optim
#
#
# class ChannelDAE(nn.Module):
#     """分通道降噪自编码器"""
#
#     def __init__(self, window_size=64):
#         super().__init__()
#         self.encoder = nn.Sequential(
#             nn.Linear(window_size, 48),
#             nn.LeakyReLU(0.1),
#             nn.Linear(48, 32),
#             nn.LeakyReLU(0.1),
#             nn.Linear(32, 16)
#         )
#
#         self.decoder = nn.Sequential(
#             nn.Linear(16, 32),
#             nn.LeakyReLU(0.1),
#             nn.Linear(32, 48),
#             nn.LeakyReLU(0.1),
#             nn.Linear(48, window_size)
#         )
#
#     def forward(self, x):
#         encoded = self.encoder(x)
#         decoded = self.decoder(encoded)
#         return decoded
#
#
# class DAE_Trainer:
#     """训练器"""
#
#     def __init__(self, channel, window_size=64):
#         self.model = ChannelDAE(window_size)
#         self.criterion = nn.HuberLoss()
#         self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-3)
#         self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
#             self.optimizer, 'min', factor=0.5, patience=5)
#         self.channel = channel
#
#     def train_epoch(self, train_loader):
#         self.model.train()
#         total_loss = 0.0
#         for inputs, _ in train_loader:
#             self.optimizer.zero_grad()
#             outputs = self.model(inputs)
#             loss = self.criterion(outputs, inputs)
#             loss.backward()
#             nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
#             self.optimizer.step()
#             total_loss += loss.item()
#         return total_loss / len(train_loader)
#
#     def validate(self, val_loader):
#         self.model.eval()
#         total_loss = 0.0
#         with torch.no_grad():
#             for inputs, _ in val_loader:
#                 outputs = self.model(inputs)
#                 loss = self.criterion(outputs, inputs)
#                 total_loss += loss.item()
#         return total_loss / len(val_loader)
import torch
import torch.nn as nn
import torch.optim as optim


class EnhancedDAE(nn.Module):
    def __init__(self, input_size=128):
        super().__init__()
        self.input_size = input_size  # 明确定义属性

        # 编码器部分保持不变
        self.encoder = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.LeakyReLU(0.2),
            nn.Linear(32, 16)
        )

        # 解码器部分保持不变
        self.decoder = nn.Sequential(
            nn.Linear(16, 32),
            nn.LeakyReLU(0.2),
            nn.Linear(32, 64),
            nn.LeakyReLU(0.2),
            nn.Linear(64, input_size)  # 确保输出尺寸匹配
        )

        # 注意力机制部分
        self.attention = nn.Sequential(
            nn.Linear(input_size, input_size),  # 输入输出维度相同
            nn.Sigmoid()
        )

    def forward(self, x):
        # 维度校验（更新后）
        assert x.dim() == 2, f"输入必须是二维张量，当前维度数：{x.dim()}"
        assert x.size(-1) == self.input_size, f"输入维度错误，预期{self.input_size}，实际{x.size(-1)}"

        # 编码-解码流程
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)

        # 注意力计算
        attn = self.attention(x)  # [batch, input_size]
        return decoded * attn + x * (1 - attn)


class DAE_Trainer:
    """增强型训练器"""

    def __init__(self, channel, input_size=128):
        self.model = EnhancedDAE(input_size)
        self.criterion = nn.SmoothL1Loss()  # 对脉冲噪声更鲁棒
        self.optimizer = optim.AdamW(self.model.parameters(),
                                     lr=1e-3, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.CyclicLR(
            self.optimizer,
            base_lr=1e-4,
            max_lr=1e-3,
            step_size_up=500,
            cycle_momentum=False
        )

    def train_epoch(self, loader):
        self.model.train()
        total_loss = 0.0
        for inputs, targets in loader:
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets)
            loss.backward()
            nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            self.scheduler.step()
            total_loss += loss.item()
            # # 更新进度条
            # loader.set_postfix(loss=loss.item())
        return total_loss / len(loader)

    def validate(self, loader):
        self.model.eval()
        total_loss = 0.0
        with torch.no_grad():
            for inputs, targets in loader:
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets)
                total_loss += loss.item()
                # # 更新进度条
                # loader.set_postfix(loss=loss.item())
        return total_loss / len(loader)
